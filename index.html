<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Stochastic nets and Bayesian regularization</title>
    <meta charset="utf-8" />
    <meta name="author" content="Joshua J Bon Queensland University of Technology Duke University– Jan 6th 2020" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="bon-beach-title.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Stochastic nets and Bayesian regularization
## <br><br>
### Joshua J Bon<br>Queensland University of Technology<br>Duke University– Jan 6th 2020

---




## Talk outline
&lt;br&gt;
1. Motivation from the (Bayesian) Lasso 
&lt;br&gt;&lt;br&gt;&lt;br&gt;
2. Stochastic constraints variables &amp; Stochastic nets
&lt;br&gt;&lt;br&gt;&lt;br&gt;
3. New priors from stochastic constraints 
&lt;br&gt;&lt;br&gt;&lt;br&gt;
4. Faster samplers for High-dimensional shrinkage
&lt;br&gt;&lt;br&gt;&lt;br&gt;
5. Strengths, weaknesses, and beyond


---

## The Lasso 

&lt;img src="imgs/aquaman-hammerhead-1967-cartoon.JPG" style="display: block; margin: auto;" /&gt;

&lt;!--- If aquaman can use a lasso under water, what can I do? "Weird" ---&gt; 

---

## The (Standard) Lasso

Optimization-based Lasso (Tibshirani, 1996):
&lt;br&gt;&lt;br&gt;&lt;br&gt;

.pull-left[.full-width[.content-box-blue[
**Lagrangian**

`$$\max_{\boldsymbol{\theta} \in \mathbb{R}^{p}} \log \pi(\boldsymbol{x}|\boldsymbol{\theta}) - \lambda \Vert \boldsymbol{\theta} \Vert_{1}$$`
]]]

.pull-right[.full-width[.content-box-red[
**Constrained**

`$$\max_{\tilde{\boldsymbol{\theta}} \in \mathbb{R}^{p}} \log \pi(\boldsymbol{x}|\tilde{\boldsymbol{\theta}}) ~\text{ s.t. }~ \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}$$`
]]]


--


- Lagrangian and constrained solutions are equal when `\(\tilde{\omega} = \Vert \boldsymbol{\theta}^{\star} \Vert_{1}\)`.



---

## The Bayesian Lasso

Probabilistic-based Lasso (Park and Casella, 2008):
&lt;br&gt;&lt;br&gt;&lt;br&gt;

.pull-left[.full-width[.content-box-blue[
**Standard**

`$$(\boldsymbol{x}|\boldsymbol{\theta}) \sim \pi(\boldsymbol{x}|\boldsymbol{\theta})$$`
`$$\boldsymbol{\theta} \overset{\text{iid}}{\sim} \text{DExp}(\lambda)$$`
]]]

--

.pull-right[.full-width[.content-box-red[
**Constrained**

`$$(\boldsymbol{x} \vert \boldsymbol{\theta}) \sim \pi(\boldsymbol{x} \vert \boldsymbol{\theta})$$`

`$$(\boldsymbol{\theta},\omega) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\omega}\;\; \text{s.t.}\;\; \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega})$$`

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad \tilde{\omega} \sim \text{Exp}(\lambda)$$`
]]]

--

- Marginalising over `\(\tilde{\omega}\)` results in the standard Bayesian Lasso.

--

- Both MAPs are equivalent to optimization-based Lasso (Lagrangian).

&lt;!--- More complicated, but exposes inner facets, Marginalising over `\(\omega\)` results in the original Bayesian Lasso. ---&gt;

---

## The regularization through constraints

Comparing the regularization:
&lt;br&gt;&lt;br&gt;&lt;br&gt;

.pull-left[.full-width[.content-box-red[
**Constrained optimization**

`$$\Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}$$`
]]]

.pull-right[.full-width[.content-box-red[
**Constrained prior**

`$$(\boldsymbol{\theta},\omega) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\omega}\;\; \text{s.t.}\;\; \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega})$$`

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad \tilde{\omega} \sim \text{Exp}(\lambda)$$`
]]]


--
&lt;br&gt;&lt;br&gt;
The pdf for the prior is
`$$p(\boldsymbol{\theta},\omega) \propto \exp(-\lambda\omega)1(\Vert \boldsymbol{\theta} \Vert_{1} \leq \omega)$$`

&lt;!--- 
	\item Closer connection between Bayesian and standard Lasso
	\item Connected by more than just the MAP
\end{itemize} ---&gt;


---

## Computation versus theory 

The SC Lasso prior is also a scale mixture of uniforms. In 1D:

.pull-left[.full-width[.content-box-red[
**Stochastic constraint**

`$$p(\theta,\omega) \propto \exp(-\lambda\omega)1(\vert \theta \vert \leq \omega)$$`
]]]

.pull-right[.full-width[.content-box-blue[
**Scale mixture of uniforms**

`$$\begin{aligned}
(\theta~\vert~\omega) &amp;\sim \text{U}(-\omega, \omega) \\
\omega &amp;\sim \text{Gam}(2,1)
\end{aligned}$$`
]]]


--

- Bayesian computation is where priors with constraints have been used
   - Slice sampling (Neal, 2003) 
   - SMU Lasso (Mallick and Yi, 2014)

--

- Gelman (2004): Re-parameterization for computation can also lead to theoretical insights (e.g. new priors)


---

## Duality and analogy for regularization
&lt;br&gt;&lt;br&gt;&lt;br&gt;

| *Inference*  | *Regularization*                                          |
|--------------|-----------------------------------------------------------|
| Optimization | Penalty `\(\Longleftrightarrow\)` constraint                  |
| Bayesian     | Prior   `\(\Longleftrightarrow\)` stochastic constraint prior |

--

&lt;br&gt;&lt;br&gt;
&lt;!--- The duality between stochastic constraint priors and their standard representation is analagous to the duality between penalty and constraint regularization in optimization.---&gt;
There is a duality in the optimization setting, 

*and for the priors in Bayesian analysis*.

---

## General stochastic constraint framework

For continuous random variables:

`$$p(\boldsymbol{\theta},\boldsymbol{\omega}) \propto f_{\tilde{\boldsymbol{\theta}}}(\boldsymbol{\theta})~f_{\tilde{\boldsymbol{\omega}}}(\boldsymbol{\omega})~1(\boldsymbol{r}(\boldsymbol{\theta}) \preceq \boldsymbol{\omega})$$`

--

|||
|---------------------------|--------------|
| Base prior               | `\(\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}}, \quad \tilde{\boldsymbol{\theta}} \in \Theta \subseteq \mathbb{R}^{p}\)`  |
| Constraint variable      | `\(\tilde{\boldsymbol{\omega}} \sim f_{\tilde{\boldsymbol{\omega}}}, \quad \tilde{\boldsymbol{\omega}} \in \Omega \subseteq \mathbb{R}^{q}\)` |
| Penalty function         | `\(\boldsymbol{r}(\boldsymbol{\theta}): \qquad \Theta \rightarrow \Omega\)` |

--

**Key ideas**: 

- Truncation is applied *jointly* to base and constraint variables.

--

- Decomposition exposes marginal and multivariate parts of the prior.

--

**Connections**: 

- SMU, SMN, slice sampling, skew RVs, exponentially tilted RVs

&lt;!---#### Base prior: 

`\(\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}}, \quad \tilde{\boldsymbol{\theta}} \in \Theta \subseteq \mathbb{R}^{p}\)`.
    
#### Constraint variable: 

`\(\tilde{\boldsymbol{\omega}} \sim f_{\tilde{\boldsymbol{\omega}}}, \quad \tilde{\boldsymbol{\omega}} \in \Omega \subseteq \mathbb{R}^{q}\)`
    
#### Penalty function: 

`\(\boldsymbol{r}(\boldsymbol{\theta}): \Theta \rightarrow \Omega\)` 


## Stochastic constraint framework

### Structure

`$$(\boldsymbol{\theta}, \boldsymbol{\omega} \vert \boldsymbol{\lambda}) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}}, \tilde{\boldsymbol{\omega}} \;\vert\;  \boldsymbol{r}(\tilde{\boldsymbol{\theta}}) \preceq \tilde{\boldsymbol{\omega}}, \boldsymbol{\lambda})$$`

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}}$$` 
`$$(\tilde{\boldsymbol{\omega}} \vert \boldsymbol{\lambda}) \sim f_{\tilde{\boldsymbol{\omega}}  \vert \boldsymbol{\lambda}}$$`

The joint support of the joint distribution `\((\tilde{\boldsymbol{\theta}}, \tilde{\boldsymbol{\omega}})\)` is constrained element-wise by the inequality `\(\boldsymbol{r}(\tilde{\boldsymbol{\theta}} ) \preceq \tilde{\boldsymbol{\omega}}\)`.--&gt;


---

## Examples

&lt;img src="imgs/aquaman-lasso2.gif" width="732" height="488" /&gt;

---

## 1D example

| Base prior | Penalty | Constraint |
|------------|---------|------------|
| `\(\text{N}(0,1)\)` | `\(-\)` | `\(-\)` |

&lt;img src="index_files/figure-html/sc-example-1-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---

## 1D example

| Base prior | Penalty | Constraint |
|------------|---------|------------|
| `\(\text{N}(0,1)\)` | `\(\vert\theta\vert\)` | `\(\text{Exp}(1)\)` |

&lt;img src="index_files/figure-html/sc-example-2-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---

## 1D example

| Base prior | Penalty | Constraint |
|------------|---------|------------|
| `\(\text{N}(0,1)\)` | `\(\vert\theta\vert\)` | `\(\text{Gamma}(0.5,1)\)` |

&lt;img src="index_files/figure-html/sc-example-3-1.png" width="600" height="400" style="display: block; margin: auto;" /&gt;

---

## The Bayesian Lasso prior (again)

Stochastic constraint priors with an exponential constraint have two main forms: 
.pull-left[.full-width[.content-box-red[
**Global**

`$$(\boldsymbol{\theta},\omega) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\omega}\;\; \text{s.t.}\;\; \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega})$$`

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad \tilde{\omega} \sim \text{Exp}(\lambda)$$`
]]]

--

.pull-right[.full-width[.content-box-red[
**Local**

`$$(\boldsymbol{\theta},\boldsymbol{\omega}) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\boldsymbol{\omega}}\;\; \text{s.t.}\;\; \vert \tilde{\theta}_{i} \vert \leq \tilde{\omega}_{i})$$`

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad \tilde{\boldsymbol{\omega}} \overset{\text{iid}}{\sim} \text{Exp}(\lambda)$$`
]]]

--

- Global uses `\(L_1\)` constraint 
- Local uses box constraint 
- Both have same marginal distribution due to memorylessness property

--

**Future work:**

- Motivates use of non-separable penalty functions

---

## The Bayesian Lasso posterior

Bayesian Lasso prior and posterior with stochastic constraints:

.pull-left[.full-width[.content-box-red[
**Prior**

`$$(\boldsymbol{\theta},\omega) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\omega}\;\; \text{s.t.}\;\; \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega})$$`

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad \tilde{\omega} \sim \text{Exp}(\lambda)$$`
]]]

--

.pull-right[.full-width[.content-box-red[
**Posterior**

`$$(\boldsymbol{\theta},\omega~\vert~\boldsymbol{y}) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\omega} \;\vert\; \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}, \boldsymbol{y})$$`

`$$(\tilde{\boldsymbol{\theta}}~\vert~\boldsymbol{y}) \sim f_{\tilde{\boldsymbol{\theta}}~\vert~\boldsymbol{y}}  \qquad \tilde{\omega} \sim \text{Exp}(\lambda)$$`
]]]

--

- Posterior is also a stochastic constraint random variable

- Learning how to sample creates posterior samplers

---

## Stochastic nets

.content-box-blue[

`$$\begin{aligned}
	\left(\boldsymbol{\theta}, \omega \vert \boldsymbol{k} \right) &amp;\overset{\text{d}}{=} \left( \tilde{\boldsymbol{\theta}}, \tilde{\omega} \mid  \Vert \tilde{\boldsymbol{\theta}} \oslash \boldsymbol{k} \Vert_{\nu}^{\nu} \leq \tilde{\omega},  \boldsymbol{k}\right) \\ 
	\tilde{\boldsymbol{\theta}} &amp;\sim f_{\tilde{\boldsymbol{\theta}}}, \quad \tilde{\boldsymbol{\theta}} \in \mathbb{R}^{p} \\ 
	 \tilde{\omega} &amp;\sim \text{Exp}(1) \\
	 \boldsymbol{k} &amp;\sim f_{\boldsymbol{k}}
\end{aligned}$$`

]

--

.pull-left[
&lt;img src="imgs/sc-rej-shape-only.png" width="288" height="240" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
- A restricted family of stochastic constraint priors
&lt;br&gt;&lt;br&gt;
- Constrained to anisotropic `\(L\text{-}\nu\)` ball with length-scale `\(\boldsymbol{k}\)`
&lt;br&gt;&lt;br&gt;
- Geometry of continuous shrinkage priors
]

---

## Visualisation of generative process

.content-box-blue[

`$$\begin{aligned}
(x,y) &amp;\sim \text{N}(0,1)\\
\boldsymbol{\lambda} &amp;\sim \text{Dir}(1,1)\\
\tau &amp;\sim \text{Exp}(2)\\
\text{s.t. }  \lambda_{1}\vert x \vert &amp;+ \lambda_{2}\vert y \vert \leq \tau
\end{aligned}$$`

]

&lt;img src="imgs/sc-rejection-viz1.gif" style="display: block; margin: auto;" /&gt;

---

## Visualisation of generative process

&lt;img src="imgs/sc-rejection-viz1-pause.png" style="display: block; margin: auto;" /&gt;

---

## Horseshoe prior as stochastic net 

.content-box-red[
`$$\begin{aligned}(\boldsymbol{\theta}, \omega \;\vert\; \boldsymbol{\lambda}) &amp;\overset{\text{d}}{=} \left( \tilde{\boldsymbol{\theta}}, \tilde{\omega} \;\vert\; \Vert\tilde{\boldsymbol{\theta}} \oslash \boldsymbol{\lambda} \Vert_{2}^{2} \leq \tilde{\omega}, \boldsymbol{\lambda} \right) \\
\tilde{\boldsymbol{\theta}} &amp;\sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \\
\tilde{\omega} &amp;\sim \text{Exp}(1/2)
\end{aligned}$$`
]

.content-box-blue[
`$$\begin{aligned}\lambda_{i} &amp;\sim \text{Cauchy}_{+}(0,1) \quad \text{for } 1 \leq i \leq p \\
\tau  &amp;\sim \text{Cauchy}_{+}(0,1)
\end{aligned}$$`
]

--

Representations exist for SNM, **Horseshoe** (Carvalho, Polson, and Scott, 2010), **Regularized-horseshoe** (Piironen and Vehtari, 2017), **Dirichlet-Laplace** (Bhattacharya, Pati, Pillai, and Dunson, 2015), **R2-D2** priors (Zhang, Reich, and Bondell, 2017)


&lt;!--- Add box around SC part to demo marginalisation?---&gt;

---
layout: false
class: inverse, middle, center

## Examples of new priors with stochastic constraints

---

## Horseshoe with informative means

Simple horseshoe prior ( `\(\tau = \sigma = 1\)` ) with flat base prior

.content-box-red[

`$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1$$`

]

--

Normal means model + horseshoe prior has shrinkage profile
.content-box-blue[

`$$\text{E}(\theta_{i}|k_{i},\boldsymbol{y}) = (1-k_{i})y_{i}$$`

]

with `\(k_{i} \sim \text{Beta}(1/2,1/2)\)`

---

## Horseshoe with informative means

Simple horseshoe prior ( `\(\tau = \sigma = 1\)` ) with informative base prior

.content-box-red[

`$$\tilde{\theta}_{i} \sim \text{N}(\mu_{i},\phi_{i}) \quad \text{for } 1 \leq i \leq p$$`

]

--

Normal means model + **informative** horseshoe prior has shrinkage profile
.content-box-blue[

`$$\text{E}(\theta_{i}|k_{i},\boldsymbol{y}) = \frac{(1-k_{i})(y_{i} + \mu_{i}\phi_{i}^{-1} )}{1 + (1-k_{i})\phi_{i}^{-1}}$$`

]

with `\(k_{i} \sim \text{Beta}(1/2,1/2)\)`

---

## Positively-skewed Horseshoe

Prior information that coefficients are positive with high probability

Full horseshoe prior with skewed (improper) base prior

.content-box-red[

`$$\tilde{\theta}_{i} \sim f_{\tilde{\theta}_{i}}(\theta) \propto \Phi(\theta/\alpha) \quad \text{for } 1 \leq i \leq p$$`

]

--

In simulations: normal regression with positive coefficients 

`\(n = 60\)`, `\(p \in \{ 50, 100, 250\}\)`:

- Reduced SSE for `\(\theta \in \{0\}\)` by factor of 2 to 4
- SSE approximately equal for  `\(\theta \in (0,0.5)\)` 
- SSE slightly worse for  `\(\theta \in [0.5,\infty)\)`
- Improved AUC marginally


---
layout: false
class: inverse, middle, center

## Faster sampling for high-dimensional continuous shrinkage priors

---

## Gibbs samplers for continuous-shrinkage priors

**Normal model**: `\((\boldsymbol{y} ~\vert~ \boldsymbol{X}, \boldsymbol{\beta},\sigma^2) \sim \text{N}(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2\boldsymbol{I})\)`.

--

### Ingredients of standard algorithm

--

(a) Sample `\((\boldsymbol{\beta}~\vert~ \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{N}(\boldsymbol{\mu},\boldsymbol{V})\)`.
   - `\(\boldsymbol{\mu} = \boldsymbol{V}\boldsymbol{X}^{\top}\boldsymbol{y}\)`
   - `\(\boldsymbol{V} = (\boldsymbol{X}^{\top}\boldsymbol{X} + \boldsymbol{S}^{-1})^{-1}\)`
   - `\(\boldsymbol{S}\)` = diagonal matrix from `\(\boldsymbol{\lambda}\)` and `\(\tau\)`.

--

(b) Sample `\((\sigma^2~\vert~\boldsymbol{\beta}, \boldsymbol{\lambda}, \tau) \sim \text{IG}(a_{1},a_{2})\)`

--

(c) Sample `\((\boldsymbol{\lambda}~\vert~\boldsymbol{\beta}, \sigma^2, \tau)\)`

--

(d) Sample `\((\tau~\vert~ \boldsymbol{\beta}, \boldsymbol{\lambda}, \sigma^2)\)`

--

**Q:** What is the computational bottleneck of this algorithm?

--

**A:** Inverting (or decomposing) `\(\boldsymbol{V}\)` is `\(\mathcal{O}(p^3)\)`.

---

### Truncated-Gibbs sampler

**Aim**: Exploit the geometry by decomposing the prior to create a better Gibbs sampler.

--

(a1) Sample `\((\boldsymbol{\omega}~\vert~\boldsymbol{\beta}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{sExp}(b_1)\)`

--

   - Use `\(\boldsymbol{\omega}\)` (current magnitude) to choose appropriate sampling step, namely

--
   
   - Order `\(\boldsymbol{\omega}\)` by size and split into two groups, `\(I\)` and `\(J\)`. Where `\(\omega_{i} \leq \epsilon\)` for `\(i \in I\)`.


--

(a2) Sample `\((\beta_{i}~\vert~ \boldsymbol{\beta}_{(i)},\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{U}(-\omega_{i}^{1/\nu},\omega_{i}^{1/\nu})\)` with MH correction for `\(i \in I\)`.

--

   - The dependence on `\(\boldsymbol{\beta}_{(i)}\)` exists but is very small. You can argue that `\((\beta_{i}~\vert~ \boldsymbol{\beta}_{(i)},\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau) \overset{d}{\approx} (\beta_{i}~\vert~\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau)\)` if `\(\epsilon\)` is small enough.

--

(a3) Sample `\((\boldsymbol{\beta}_{J}~\vert~ \boldsymbol{\beta}_{I}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{N}(\boldsymbol{\mu}_{J|I},\boldsymbol{V}_{J|I})\)`.

--

   - Note: `\(\boldsymbol{\omega}\)` marginalised out. Not necessary, but practical.


---

### Truncated-Gibbs sampler

**Aim**: Exploit the geometry by decomposing the prior to create a better Gibbs sampler.

(a1) Sample `\((\boldsymbol{\omega}~\vert~\boldsymbol{\beta}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{sExp}(b_1)\)`

(a2) Sample `\((\beta_{i}~\vert~ \boldsymbol{\beta}_{(i)},\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{U}(-\omega_{i}^{1/\nu},\omega_{i}^{1/\nu})\)` with MH correction for `\(i \in I\)`.

(a3) Sample `\((\boldsymbol{\beta}_{J}~\vert~ \boldsymbol{\beta}_{I}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{N}(\boldsymbol{\mu}_{J|I},\boldsymbol{V}_{J|I})\)`.

--

**Complexity**: 

- `\(\mathcal{O}(\vert J \vert^{3} + p \vert I\vert)\)` in each iteration.

--

- We can fix `\(\vert J \vert\)` by selecting `\(\epsilon\)`. Then the complexity is `\(\mathcal{O}(p^2)\)`.

---

### Truncated-Gibbs sampler summary

- In high dimensions with shrinkage priors, the majority of coefficients are forced to be very close to zero.
 
--
 
- Understanding the structure and geometry of the problem is important!


--

&lt;br&gt;&lt;br&gt;
&lt;img src="imgs/aquaman-jetski.gif" style="display: block; margin: auto;" /&gt;

---

## Results on some toy problems

Model: Linear regression, iid Gaussian errors, using R2-D2 prior

Simulation settings:

`\(n = 100\)`

`\(p \in \{500,1000,2500,5000\}\)`

`\(\rho \in \{0.1,0.5\}\)`, controls correlation in `\(\boldsymbol{X}\)`

`\(\epsilon \in \{0.1,0.2\}\)`

`\(\boldsymbol{\beta} = [\boldsymbol{0}_{10}^{\top}~\boldsymbol{b}_{1}^{\top}~\boldsymbol{0}_{30}^{\top}~~\boldsymbol{b}_{2}^{\top}~\boldsymbol{0}_{p-50}^{\top}]^{\top}\)`

`\(\boldsymbol{b}_{1} = [2,~2,−5,−5,−5]\)`

`\(\boldsymbol{b}_{2} = [−2,−2,−2,~5,~5]\)`

`\(2000\)` samples repeated independently 100 times.

---

### Mean (SD) computation time

&lt;img src="imgs/avg-computation-time.png" width="500" height="500" style="display: block; margin: auto;" /&gt;

---

### Posterior mean (SD) of coefficient SSE

&lt;img src="imgs/table-sse.png" height="500" style="display: block; margin: auto;" /&gt;

---

### Grouping and uniform proposal

&lt;img src="imgs/table-mh-accept.png" width="550" height="500" style="display: block; margin: auto;" /&gt;

&lt;!---### Mean (SD) computation time

&lt;img src="imgs/table-time.png" width="450" height="550" style="display: block; margin: auto;" /&gt;
---&gt;

---
layout: false
class: inverse, middle, center

## Strengths, weaknesses, and beyond

---

## Strengths of SCP

**More views on shrinkage**
&lt;br&gt;
- Geometry of stochastic constraints
- Stochastic constraints as rare events
- A type of "conjugacy" for posterior
&lt;br&gt;&lt;br&gt;

--

**Very flexible**
&lt;br&gt;
- Base, constraining RVs
- Penalty function
- New priors for high dimensions

---

## Weaknesses of SCP

**In general hard to sample**
&lt;br&gt;
- Rare event simulation
- Difficult normalising constant in prior
&lt;br&gt;

`$$p(\boldsymbol{\theta},\boldsymbol{\omega}) \propto f_{\tilde{\boldsymbol{\theta}}}(\boldsymbol{\theta})~f_{\tilde{\boldsymbol{\omega}}}(\boldsymbol{\omega})~1(\boldsymbol{r}(\boldsymbol{\theta}) \preceq \boldsymbol{\omega})$$`

&lt;br&gt;

--

**Preference to avoid constraints**
&lt;br&gt;&lt;br&gt;

--

**Theory more cumbersome with constraints**

---

## Current/future work

Other areas under investigation with stochastic constraints:

- Exact sampler for SC (e.g. Bayesian Lasso)
   
- Theoretical results
   
- Discrete version

- Prior construction

---

### Acknowledgements

   - Masters supervisors: Berwin Turlach, Kevin Murray, Chris Drovandi
   &lt;br&gt;&lt;br&gt;
   - University of Western Australia, Pawsey Supercomputing Centre
   &lt;br&gt;&lt;br&gt;
   - Queensland University of Technology &amp; ACEMS
   &lt;br&gt;&lt;br&gt;
   - Duke University &amp; David Dunson

---
class: middle

.pull-left[
&lt;img src="imgs/aquaman-flying-fish.gif" width="350" height="258" style="display: block; margin: auto;" /&gt;
]

.pull-right[
**Thank you**
&lt;br&gt;&lt;br&gt;&lt;br&gt;
Twitter: `@bonstats`
&lt;br&gt;&lt;br&gt;&lt;br&gt;
Email: `joshuajbon@gmail.com`

]


---

## References

&lt;p&gt;&lt;cite&gt;Bhattacharya, A. et al.
(2015).
&amp;ldquo;Dirichlet-Laplace priors for optimal shrinkage&amp;rdquo;.
In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 110.512, pp. 1479&amp;ndash;1490.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Carvalho, C. M. et al.
(2010).
&amp;ldquo;The horseshoe estimator for sparse signals&amp;rdquo;.
In: &lt;em&gt;Biometrika&lt;/em&gt; 97.2, pp. 465&amp;ndash;480.
DOI: &lt;a href="https://doi.org/10.1093/biomet/asq017"&gt;10.1093/biomet/asq017&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Gelman, A.
(2004).
&amp;ldquo;Parameterization and Bayesian modeling&amp;rdquo;.
In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 99.466, pp. 537&amp;ndash;545.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Mallick, H. et al.
(2014).
&amp;ldquo;A New Bayesian Lasso.&amp;rdquo;
In: &lt;em&gt;Statistics and its interface&lt;/em&gt; 7.4, pp. 571&amp;ndash;582.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Neal, R. M.
(2003).
&amp;ldquo;Slice sampling&amp;rdquo;.
In: &lt;em&gt;Annals of Statistics&lt;/em&gt; 31.3, pp. 705&amp;ndash;767.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Park, T. et al.
(2008).
&amp;ldquo;The Bayesian Lasso&amp;rdquo;.
In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 103.482, pp. 681&amp;ndash;686.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Piironen, J. et al.
(2017).
&amp;ldquo;Sparsity information and regularization in the horseshoe and other shrinkage priors&amp;rdquo;.
In: &lt;em&gt;Electronic Journal of Statistics&lt;/em&gt; 11.2, pp. 5018&amp;ndash;5051.
DOI: &lt;a href="https://doi.org/10.1214/17-ejs1337si"&gt;10.1214/17-ejs1337si&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Tibshirani, R.
(1996).
&amp;ldquo;Regression shrinkage and selection via the lasso&amp;rdquo;.
In: &lt;em&gt;Journal of the Royal Statistical Society. Series B (Methodological)&lt;/em&gt;, pp. 267&amp;ndash;288.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Zhang, Y. et al.
(2017).
&amp;ldquo;High dimensional linear regression via the R2-D2 shrinkage prior&amp;rdquo;.
In: &lt;em&gt;arXiv preprint arXiv:1609.00046v2&lt;/em&gt;.&lt;/cite&gt;&lt;/p&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
